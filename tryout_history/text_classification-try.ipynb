{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_r/s4ymnyp94kg4jf8pzqddcwmh0000gn/T/jieba.cache\n",
      "Loading model cost 0.969 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 给/分/溢出/了/，/是/怎么回事/呢/什么/是/ /Philo/ /-/ /sophia/ /？/ /以下/是/一些/“/大/问题/”/，/请/选择/一定/的/立场/，/并/为/这/一/立场/做/辩护/：/上帝/是否/存在/？/人能/不能/两次/踏进/同/一条/河流/？/空间/究竟/空不空/？/“/杯子/”/为什么/是/“/商品/”/？/ /Philosophia/ /讲/不/讲/ /Logic/？/ / / / / / / / /“/物质/是/运动/的/”/ /-/>/ /“/运动/是/有/规律/的/”/ /-/>/ /“/规律/是/可以/被/把握/的/”/ /-/>/ /“/因为/人/有意识/”/ /-/>/ /“/且/意识/具有/主观/能动性/”/ /-/>/ /“/但/意识/终究/离不开/物质/及其/运动/规律/”/ /-/>/ /“/因此/物质/决定/意识/”/ /算不算/有/逻辑/？/ / / / / / / /怎样/从/逻辑/上/推出/资本主义/金融危机/和/“/两个/必然/”/的/？/ / / / / / / /这个/过程/和/自然科学/通过/设定/一系列/假设/（/比如/欧几里得/五条/公设/，/或/量子/物理/的/波函数/假设/）/，/然后/推出/后面/的/东西/有/一定/的/相似之处/—/—/毕竟/经典力学/的/开山/之作/也/叫/《/自然哲学/的/数学原理/》/。/这样一来/也/不难理解/哲学/有/各种/派别/的/争论/，/因为/他们/的/基本/假设/不/一样/。/数学家/将/第五/公设/稍作/更改/，/创立/的/ /Riemann/ /几何/在/欧氏/几何/看来/有/一系列/的/矛盾/，/也/是/如此/。/至于/量子/物理/，/它/不/符合/人们/的/直觉/，/但/并/不/代表/着/它/的/逻辑/不/顺畅/。/ / / / / / / /马克思/理论/的/基本/假设/，/是/商品/理论/和/劳动价值论/。/作为/对比/，/新/自由主义/和/凯恩斯主义/这/两个/经济/理论/的/基本/假设/就/不同/，/反倒/和/物理/上/的/统计力学/非常/相似/，/也/搞/ /Boltzmann/ /分布/（/但/马克思主义/暂时/还/没有/找到/一个/能够/类比/的/物理/模型/）/。/剩下/的/大多/是/逻辑/推导/。/比如/像/下面/这样/：/ / / / / / / /定义/：/称/一个/理论/是/好/理论/（/good/_/theory/）/，/当且/仅/当/它/逻辑/自洽/，/能够/帮助/我们/理解/世界/的/运行/机理/，/能够/预测/即将/发生/的/情况/。/ / / / / / / /定理/一/：/好/理论/的/“/好度/”/（/goodity/）/是/它/自洽/程度/、/预测/精度/的/一个/度量/，/它/是/时间/的/单调/函数/：/\\/(/\\/text/{/goodity/}/(/t/)/\\/)/ / / / / / / /定义/：/真理/是/一系列/好/理论/在/好度/趋于/无穷/时/的/极限/：/ / / / / / / /\\/(/\\/lim/_/{/t/\\/rightarrow/\\/infty/}/ /\\/text/{/good/_/theory/}/=/ /\\/text/{/truth/}/\\/)/ / / / / / / /推论/一/：/历史/地看/，/马克思主义/是/一个/好/理论/，/而/不是/真理/。/ / / / / / / /当/“/理论/”/空间/有/更/高/的/维度/，/人们/选取/的/理论/列/可以/从/各种/方向/趋近/这个/极限/，/那/就/不难理解/：/ / / / / / / /推论/二/：/真理/存在/于/争论/之中/。/ /思/-/政/，/怎样/ /trade/-/off/ /？/ / / / / / / /假如/我们/的/日常/思维/是/一种/有/可能/不/那么/好/的/习惯/，/而/绝大部分/人/哪怕/是/很多/高级/知识分子/都/不/自知/，/是否是/一件/可怕/的/事情/？/ / / / / / / /这时/，/如果/有/一些/人/通过/合适/的/导引/成为/被/撕裂/的/自/在/的/人/，/那么/在/这个/探索/中/，/他会/不断/向/一个/丰富/的/自为/的/人/迈进/。/因为/，/那些/人/感受/到/了/差别/并/开始/思考/。/所以/黑客帝国/的/主角/叫/“/neo/”/，/重排/则/是/“/one/”/。/ / / / / / / /所以/：/ / / / / / / /“/上/完/这门/课后/，/才/是/真正/人文/教育/的/开始/。/好/的/人文/课程/只是/个/导引/，/不能/当作/全部/，/更/不能/当作/绝对/结论/，/这/只是/我们/一生/探索/的/出发点/。/”/ / / / / / / /“/因为/你/会/自己/探索/阅读/了/，/不再/是/小白/了/，/你/成/了/一个/有/规定性/的/有/—/—/运动/的/定/在/，/而/不是/纯粹/的/不变/的/存在/。/”/ / / / / / / /人文/教育/，/任重道远/。/ / / / / / / /一门/课/、/一个/老师/究竟/怎么样/，/同学/们/都/是/有目共睹/的/。/对/我/来说/，/2021/ /年/的/秋冬/，/能上/这门/课/是/我/人生/中/的/重大/转折/：/无论是/在/思维/方式/层面/还是/思想境界/层面/，/我/发现/从前/一团/糊状/的/逻辑/链条/变得/清晰/明白/；/从前/无法/理解/的/高深/概念/变得/和蔼可亲/。/甚至/在/遣词造句/、/章法/结构/上/，/都/有/了/很大/的/提升/。/我/发现/我能/阅读/很长/很长/的/文科/段落/而/不致/困倦/，/我/发现/我能/挑出/很深/很/深/的/论述/毛病/并/予以/反击/。/ / / / / / / /有/一天/颜/老师/问/我/是否/能/理解/这样/上课/，/而且/是/上/一门/“/思政水课/”/的/用意/，/我点/了/点头/—/—/这/不是/可怜/的/、/同情/的/理解/，/而是/钦佩/的/、/仰慕/的/理解/—/—/让/我/不断/向/一个/自在/自为/的/人/迈进/。/也/很/羡慕/老师/能够/研究/自己/热爱/的/方向/，/上课/常常/能/满怀/激动/地/从/15/:/55/一直/不停/地/讲/到/18/:/20/（/只有/教学/督导/能/让/他/课间休息/五分钟/，/不过/好像/ /2022/ /年/没有/这样/了/）/。/ / / / / / / /以后/谁/要是/说/我/出身/科大/没有/人文/情怀/，/我/就/把/颜/老师/的/PPT/和/我/的/小/作业/大/论文/鞣/在/一起/，/一并/扔/他/脸上/。/ /8/月/22/日/更新/：/经颜/老师/指导/，/给/新生/做/了/报告/，/分享/一张/板书/：/ /这/是/讲/哲学/认识论/中/ /self/ /和/ /other/ /的/作用/机理/时/留下/的/，/但是/有/一些/错误/：/左上角/对群/的/定义/少/了/单位/元/的/存在/性/和/逆/的/存在/性/两条/黑火药/一硫二硝三/木炭/写成/了/一/磷二硝三/木炭/儒教/新教/的/英文/是/ /protestantism/ /左下角/写/的/是/三十年/战争/《/新教/伦理/与/资本主义/精神/》/ /12/月/4/日/更新/：/有人/在/力学/分享/会上/讲/哲学/，/但/我/不/说/是/谁/./jpg/更正/一些/错误/：/Logos/ /一般/写为/“/逻各斯/”/ /讲稿/请/点击/这里/：/《/对/中国/科学技术/大学/的/分析/》/ /[/1/]/./ /颜惠箭/，/马克思主义/基本原理/，/2021/秋/、/2022/秋/[/2/]/./ /邓晓芒/，/中西文化/心理/比较/讲演录/，/人民出版社/，/2013/[/3/]/./ /邓晓芒/，/儒家/伦理/新/批判/，/重庆大学/出版社/，/2010/[/4/]/./罗伯特/·/所罗门/，/大/问题/：/简明/哲学/导论/，/清华大学出版社/，/2018/[/5/]/./ /爱因斯坦/，/我/的/世界观/，/商务印书馆/，/2018/[/6/]/./ /理查德/·/费曼/，/费曼/物理学/讲义/，/上海/科学技术/出版社/，/2020/[/7/]/./ /程艺/老师/的/评课/社区/（/数学分析/B1/）/[/8/]/./ /狄拉克/，/量子力学/原理/，/机械/工业/出版社/，/2018/[/9/]/./ /上海交通大学/生存/手册/[/10/]/./ /康德/，/实践/理性/批判/，/商务印书馆/，/2021/[/11/]/./Drifting/ /Soul/ /同学/的/点评/[/12/]/./知乎/：/如何/超越/牛顿/、/爱因斯坦/，/成为/人类/历史/上/最/伟大/的/物理学家/？/ /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut('给分溢出了，是怎么回事呢什么是 Philo - sophia\\xa0？\\xa0以下是一些“大问题”，请选择一定的立场，并为这一立场做辩护：上帝是否存在？人能不能两次踏进同一条河流？空间究竟空不空？“杯子”为什么是“商品”？\\xa0Philosophia\\xa0讲不讲 Logic？\\xa0\\xa0 \\xa0 \\xa0 \\xa0“物质是运动的” ->\\xa0“运动是有规律的”\\xa0->\\xa0“规律是可以被把握的”\\xa0->\\xa0“因为人有意识”\\xa0->\\xa0“且意识具有主观能动性”\\xa0->\\xa0“但意识终究离不开物质及其运动规律”\\xa0->\\xa0“因此物质决定意识” 算不算有逻辑？\\xa0 \\xa0 \\xa0 \\xa0怎样从逻辑上推出资本主义金融危机和“两个必然”的？\\xa0 \\xa0 \\xa0 \\xa0这个过程和自然科学通过设定一系列假设（比如欧几里得五条公设，或量子物理的波函数假设），然后推出后面的东西有一定的相似之处——毕竟经典力学的开山之作也叫《自然哲学的数学原理》。这样一来也不难理解哲学有各种派别的争论，因为他们的基本假设不一样。数学家将第五公设稍作更改，创立的\\xa0Riemann 几何在欧氏几何看来有一系列的矛盾，也是如此。至于量子物理，它不符合人们的直觉，但并不代表着它的逻辑不顺畅。\\xa0 \\xa0 \\xa0 \\xa0马克思理论的基本假设，是商品理论和劳动价值论。作为对比，新自由主义和凯恩斯主义这两个经济理论的基本假设就不同，反倒和物理上的统计力学非常相似，也搞 Boltzmann 分布（但马克思主义暂时还没有找到一个能够类比的物理模型）。剩下的大多是逻辑推导。比如像下面这样：\\xa0 \\xa0 \\xa0 \\xa0定义：称一个理论是好理论（good_theory），当且仅当它逻辑自洽，能够帮助我们理解世界的运行机理，能够预测即将发生的情况。\\xa0 \\xa0 \\xa0 \\xa0定理一：好理论的“好度”（goodity）是它自洽程度、预测精度的一个度量，它是时间的单调函数：\\\\(\\\\text{goodity}(t)\\\\)\\xa0 \\xa0 \\xa0 \\xa0定义：真理是一系列好理论在好度趋于无穷时的极限：\\xa0 \\xa0 \\xa0 \\xa0\\\\(\\\\lim_{t\\\\rightarrow\\\\infty} \\\\text{good_theory}= \\\\text{truth}\\\\)\\xa0 \\xa0 \\xa0 \\xa0推论一：历史地看，马克思主义是一个好理论，而不是真理。\\xa0 \\xa0 \\xa0 \\xa0当“理论”空间有更高的维度，人们选取的理论列可以从各种方向趋近这个极限，那就不难理解：\\xa0 \\xa0 \\xa0 \\xa0推论二：真理存在于争论之中。\\xa0思-政，怎样\\xa0trade-off\\xa0？\\xa0\\xa0 \\xa0 \\xa0\\xa0假如我们的日常思维是一种有可能不那么好的习惯，而绝大部分人哪怕是很多高级知识分子都不自知，是否是一件可怕的事情？\\xa0\\xa0 \\xa0 \\xa0\\xa0这时，如果有一些人通过合适的导引成为被撕裂的自在的人，那么在这个探索中，他会不断向一个丰富的自为的人迈进。因为，那些人感受到了差别并开始思考。所以黑客帝国的主角叫“neo”，重排则是“one”。\\xa0 \\xa0 \\xa0 \\xa0所以：\\xa0 \\xa0 \\xa0 \\xa0“上完这门课后，才是真正人文教育的开始。好的人文课程只是个导引，不能当作全部，更不能当作绝对结论，这只是我们一生探索的出发点。”\\xa0 \\xa0 \\xa0 \\xa0“因为你会自己探索阅读了，不再是小白了，你成了一个有规定性的有——运动的定在，而不是纯粹的不变的存在。”\\xa0 \\xa0 \\xa0 \\xa0人文教育，任重道远。\\xa0 \\xa0 \\xa0 \\xa0一门课、一个老师究竟怎么样，同学们都是有目共睹的。对我来说，2021 年的秋冬，能上这门课是我人生中的重大转折：无论是在思维方式层面还是思想境界层面，我发现从前一团糊状的逻辑链条变得清晰明白；从前无法理解的高深概念变得和蔼可亲。甚至在遣词造句、章法结构上，都有了很大的提升。我发现我能阅读很长很长的文科段落而不致困倦，我发现我能挑出很深很深的论述毛病并予以反击。\\xa0 \\xa0 \\xa0 \\xa0有一天颜老师问我是否能理解这样上课，而且是上一门“思政水课”的用意，我点了点头——这不是可怜的、同情的理解，而是钦佩的、仰慕的理解——让我不断向一个自在自为的人迈进。也很羡慕老师能够研究自己热爱的方向，上课常常能满怀激动地从15:55一直不停地讲到18:20（只有教学督导能让他课间休息五分钟，不过好像 2022\\xa0年没有这样了）。\\xa0\\xa0 \\xa0 \\xa0\\xa0以后谁要是说我出身科大没有人文情怀，我就把颜老师的PPT和我的小作业大论文鞣在一起，一并扔他脸上。\\xa08月22日更新：经颜老师指导，给新生做了报告，分享一张板书：\\xa0这是讲哲学认识论中 self 和 other 的作用机理时留下的，但是有一些错误：左上角对群的定义少了单位元的存在性和逆的存在性两条黑火药一硫二硝三木炭写成了一磷二硝三木炭儒教新教的英文是\\xa0protestantism\\xa0左下角写的是三十年战争《新教伦理与资本主义精神》\\xa012月4日更新：有人在力学分享会上讲哲学，但我不说是谁.jpg更正一些错误：Logos 一般写为“逻各斯”\\xa0讲稿请点击这里：《对中国科学技术大学的分析》\\xa0[1]. 颜惠箭，马克思主义基本原理，2021秋、2022秋[2]. 邓晓芒，中西文化心理比较讲演录，人民出版社，2013[3]. 邓晓芒，儒家伦理新批判，重庆大学出版社，2010[4].罗伯特·所罗门，大问题：简明哲学导论，清华大学出版社，2018[5]. 爱因斯坦，我的世界观，商务印书馆，2018[6]. 理查德·费曼，费曼物理学讲义，上海科学技术出版社，2020[7]. 程艺老师的评课社区（数学分析B1）[8]. 狄拉克，量子力学原理，机械工业出版社，2018[9]. 上海交通大学生存手册[10]. 康德，实践理性批判，商务印书馆，2021[11].Drifting Soul 同学的点评[12].知乎：如何超越牛顿、爱因斯坦，成为人类历史上最伟大的物理学家？\\xa0\\n', cut_all=False)\n",
    "print(\"Default Mode:\", \"/\".join(seg_list))  # Default Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Import the Hugging Face library\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained RoBERTa tokenizer and model for Chinese\n",
    "tokenizer = RobertaTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = RobertaForSequenceClassification.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "\n",
    "# Encode a pair of sentences and make a prediction\n",
    "sentence1 = \"这部电影很好看，我很喜欢。\" # This movie is very good, I like it very much.\n",
    "sentence2 = \"这部电影很无聊，我很讨厌。\" # This movie is very boring, I hate it very much.\n",
    "input_ids = tokenizer.encode(sentence1, sentence2, return_tensors='pt')\n",
    "logits = model(input_ids)[0]\n",
    "prediction = torch.argmax(logits).item()\n",
    "print(prediction) # 0 (contradiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = trans.pipeline('sentiment-analysis')\n",
    "classifier(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8317492604255676}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb 单元格 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the pre-trained RoBERTa tokenizer and model for Chinese\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m trans\u001b[39m.\u001b[39;49mRobertaTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mhfl/chinese-roberta-wwm-ext\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m trans\u001b[39m.\u001b[39mRobertaForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mhfl/chinese-roberta-wwm-ext\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1804\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1804\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1805\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1806\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1807\u001b[0m     init_configuration,\n\u001b[1;32m   1808\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1809\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1810\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1811\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1812\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1813\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1814\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1958\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1959\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/tokenization_roberta.py:226\u001b[0m, in \u001b[0;36mRobertaTokenizer.__init__\u001b[0;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[1;32m    213\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    214\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    215\u001b[0m     bos_token\u001b[39m=\u001b[39mbos_token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    224\u001b[0m )\n\u001b[0;32m--> 226\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(vocab_file, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(vocab_handle)\n\u001b[1;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained RoBERTa tokenizer and model for Chinese\n",
    "tokenizer = trans.RobertaTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "model = trans.RobertaForSequenceClassification.from_pretrained('hfl/chinese-roberta-wwm-ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b780b905e347e597a4e859f3e851bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5293f3f5f82d4408a78bb4b03bb413c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb 单元格 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sumamrize \u001b[39m=\u001b[39m trans\u001b[39m.\u001b[39;49mpipeline(task\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msummarization\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/__init__.py:776\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39m# Infer the framework from the model\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[39m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39m# Will load the correct model if possible\u001b[39;00m\n\u001b[1;32m    775\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 776\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    777\u001b[0m     model,\n\u001b[1;32m    778\u001b[0m     model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    779\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    780\u001b[0m     framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    781\u001b[0m     task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    782\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    783\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    784\u001b[0m )\n\u001b[1;32m    786\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    787\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/pipelines/base.py:271\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m with any of the following classes: \u001b[39m\u001b[39m{\u001b[39;00mclass_tuple\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m framework \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mkeras.engine.training.Model\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(inspect\u001b[39m.\u001b[39mgetmro(model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m \u001b[39mreturn\u001b[39;00m framework, model\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model sshleifer/distilbart-cnn-12-6 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.bart.modeling_bart.BartForConditionalGeneration'>)."
     ]
    }
   ],
   "source": [
    "sumamrize = trans.pipeline(task=\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b417bb88fa234523aca12c7ed72d1abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2deb4ff1a749f78b67371e28e089f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6307b6fce244c7849b6200f438836f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba38dfc45754227a47b94a654852b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb 单元格 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the pre-trained RoBERTa tokenizer and model for Chinese\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m trans\u001b[39m.\u001b[39;49mRobertaTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mhfl/chinese-roberta-wwm-ext\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m trans\u001b[39m.\u001b[39mRobertaForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mhfl/chinese-roberta-wwm-ext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/royalty/Desktop/Python_ML/Python-DL/Web-Python/text-try.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Encode a pair of sentences and make a prediction\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1804\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1804\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1805\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1806\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1807\u001b[0m     init_configuration,\n\u001b[1;32m   1808\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1809\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1810\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1811\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1812\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1813\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1814\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1958\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1959\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/tokenization_roberta.py:226\u001b[0m, in \u001b[0;36mRobertaTokenizer.__init__\u001b[0;34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[1;32m    213\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    214\u001b[0m     errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    215\u001b[0m     bos_token\u001b[39m=\u001b[39mbos_token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    224\u001b[0m )\n\u001b[0;32m--> 226\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(vocab_file, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(vocab_handle)\n\u001b[1;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m {v: k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encode a pair of sentences and make a prediction\n",
    "sentence1 = \"这部电影很好看，我很喜欢。\" # This movie is very good, I like it very much.\n",
    "sentence2 = \"这部电影很无聊，我很讨厌。\" # This movie is very boring, I hate it very much.\n",
    "input_ids = tokenizer.encode(sentence1, sentence2, return_tensors='pt')\n",
    "logits = model(input_ids)[0]\n",
    "prediction = torch.argmax(logits).item()\n",
    "print(prediction) # 0 (contradiction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
